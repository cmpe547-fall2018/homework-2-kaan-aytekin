{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Kaan Aytekin\n",
    "\n",
    "I hereby declare that I observed the honour code of the university when preparing the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr?gr?mm?ng?H?m?w?rk\n",
    "\n",
    "In this exercise we model a string of text using a Markov(1) model. For simplicity we only consider letters 'a-z'. Capital letters 'A-Z' are mapped to the corresponding ones. All remaining letters, symbols, numbers, including spaces, are denoted by '.'.\n",
    "\n",
    "\n",
    "We have a probability table $T$ where $T_{i,j} = p(x_t = j | x_{t-1} = i)$  transition model of letters in English text for $t=1,2 \\dots N$. Assume that the initial letter in a string is always a space denoted as $x_0 = \\text{'.'}$. Such a model where the probability table is always the same is sometimes called a stationary model.\n",
    "\n",
    "1. For a given $N$, write a program to sample random strings with letters $x_1, x_2, \\dots, x_N$ from $p(x_{1:N}|x_0)$\n",
    "1. Now suppose you are given strings with missing letters, where each missing letter is denoted by a question mark (or underscore, as below). Implement a method, that samples missing letters conditioned on observed ones, i.e., samples from $p(x_{-\\alpha}|x_{\\alpha})$ where $\\alpha$ denotes indices of observed letters. For example, if the input is 't??.', we have $N=4$ and\n",
    "$x_1 = \\text{'t'}$ and $x_4 = \\text{'.'}$, $\\alpha=\\{1,4\\}$ and $-\\alpha=\\{2,3\\}$. Your program may possibly generate the strings 'the.', 'twi.', 'tee.', etc. Hint: make sure to make use all data given and sample from the correct distribution. Implement the method and print the results for the test strings below. \n",
    "1. Describe a method for filling in the gaps by estimating the most likely letter for each position. Hint: you need to compute\n",
    "$$\n",
    "x_{-\\alpha}^* = \\arg\\max_{x_{-\\alpha}} p(x_{-\\alpha}|x_{\\alpha})\n",
    "$$\n",
    "Implement the method and print the results for the following test strings along with the log-probability  $\\log p(x_{-\\alpha}^*,x_{\\alpha})$.\n",
    "1. Discuss how you can improve the model to get better estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: The code below loads a table of transition probabilities for English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949749\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t | x_{t-1} = \\text{'a'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.0002835\n",
      "b 0.0228302\n",
      "c 0.0369041\n",
      "d 0.0426290\n",
      "e 0.0012216\n",
      "f 0.0075739\n",
      "g 0.0171385\n",
      "h 0.0014659\n",
      "i 0.0372661\n",
      "j 0.0002353\n",
      "k 0.0110124\n",
      "l 0.0778259\n",
      "m 0.0260757\n",
      "n 0.2145354\n",
      "o 0.0005459\n",
      "p 0.0195213\n",
      "q 0.0001749\n",
      "r 0.1104770\n",
      "s 0.0934290\n",
      "t 0.1317960\n",
      "u 0.0098029\n",
      "v 0.0306574\n",
      "w 0.0088799\n",
      "x 0.0009562\n",
      "y 0.0233701\n",
      "z 0.0018701\n",
      ". 0.0715219\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T = []\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        T.append(row)\n",
    "\n",
    "print('Example')\n",
    "## p(x_t = 'u' | x_{t-1} = 'q')\n",
    "display(Latex(r\"$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$\"))\n",
    "print(T[letter2idx['q']][letter2idx['u']])\n",
    "display(Latex(r\"$p(x_t | x_{t-1} = \\text{'a'})$\"))\n",
    "for c,p in zip(alphabet,T[letter2idx['a']]):\n",
    "    print(c,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import itertools\n",
    "TMatrix=np.matrix(T,dtype=float)\n",
    "FirstLetterProbs=T[len(T)-1]\n",
    "def RandomTextGenerator(N):\n",
    "\tword=(N+1)*['.']\n",
    "\tletter0='.'\n",
    "\tpreviousLetter=letter0\n",
    "\tfor i in range(N):\n",
    "\t\trandomNumber=np.random.random()\n",
    "\t\tTransitionProbs=T[letter2idx[previousLetter]]\n",
    "\t\tcumulativeDist=np.cumsum(sorted(TransitionProbs))\n",
    "\t\tfor j in range(27):\n",
    "\t\t\tif cumulativeDist[j]>randomNumber:\n",
    "\t\t\t\tselectedLetterIndex=j\n",
    "\t\t\t\tbreak\n",
    "\t\tnextLetterIndex=np.argsort(TransitionProbs)[j]\n",
    "\t\tnextLetter=list(letter2idx.keys())[nextLetterIndex]\n",
    "\t\tword[i+1]=nextLetter\n",
    "\t\tpreviousLetter=nextLetter\n",
    "\treturn(''.join(word))\n",
    "\n",
    "def TextSampler(test_strings,PredictionMultiplier):\n",
    "\tfor text in test_strings:\n",
    "\t\tfor times in range(PredictionMultiplier):\t\n",
    "\t\t\tknownLetterIndeces=[i for i,letter in enumerate(text) if letter!='_']\n",
    "\t\t\t#knownLetterIndeces={i:letter for i,letter in enumerate(text) if letter!='_'}\n",
    "\t\t\tunknownLetterIndeces=[i for i,letter in enumerate(text) if letter=='_']\n",
    "\t\t\t#unknownLetterIndeces={i:letter for i,letter in enumerate(text) if letter=='_'}\n",
    "\t\t\tfoundLetters={}\n",
    "\t\t\tfor i,letter in enumerate(text):\n",
    "\t\t\t\tif i in unknownLetterIndeces:\n",
    "\t\t\t\t\tif i==0:\n",
    "\t\t\t\t\t\tfirstKnownLetterIndex=knownLetterIndeces[0]\n",
    "\t\t\t\t\t\tTransitionMat=np.identity(len(T))\n",
    "\t\t\t\t\t\tfor _ in range(firstKnownLetterIndex):\n",
    "\t\t\t\t\t\t\tTransitionMat=np.matmul(TransitionMat,TMatrix)\n",
    "\t\t\t\t\t\tLikelihoodTimesPrior=np.multiply(TMatrix[len(T)-1,:],np.transpose(TransitionMat[:,letter2idx[text[firstKnownLetterIndex]]]))\n",
    "\t\t\t\t\t\tEvidence=np.matmul(TransitionMat,TMatrix)[len(T)-1,letter2idx[text[firstKnownLetterIndex]]]\n",
    "\t\t\t\t\t\tPosterior=LikelihoodTimesPrior/Evidence\n",
    "\t\t\t\t\t\tcumulativeDist=np.cumsum(sorted(Posterior))\n",
    "\t\t\t\t\t\trandomNumber=np.random.random()\n",
    "\t\t\t\t\t\tfor _ in range(len(T)):\n",
    "\t\t\t\t\t\t\tif cumulativeDist[_]>randomNumber:\n",
    "\t\t\t\t\t\t\t\tselectedLetterIndex=_\n",
    "\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\tfoundLetters[i]=(list(letter2idx.keys())[selectedLetterIndex])\n",
    "\t\t\t\t\t\t#knownLetterIndeces.append(i)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tif (i-1) in knownLetterIndeces:\n",
    "\t\t\t\t\t\t\tnextKnownLetterIndex=[nextIndex for nextIndex in knownLetterIndeces if nextIndex>i]\n",
    "\t\t\t\t\t\t\tif len(nextKnownLetterIndex)>0:\n",
    "\t\t\t\t\t\t\t\tnextKnownLetterIndex=nextKnownLetterIndex[0]\n",
    "\t\t\t\t\t\t\t\tTransitionMat=np.identity(len(T))\n",
    "\t\t\t\t\t\t\t\tfor _ in range(nextKnownLetterIndex-i):\n",
    "\t\t\t\t\t\t\t\t\tTransitionMat=np.matmul(TransitionMat,TMatrix)\n",
    "\t\t\t\t\t\t\t\tLikelihoodTimesPrior=np.multiply(TMatrix[letter2idx[text[(i-1)]],:],np.transpose(TransitionMat[:,letter2idx[text[nextKnownLetterIndex]]]))\n",
    "\t\t\t\t\t\t\t\tEvidence=np.matmul(TransitionMat,TMatrix)[letter2idx[text[(i-1)]],letter2idx[text[nextKnownLetterIndex]]]\n",
    "\t\t\t\t\t\t\t\tPosterior=LikelihoodTimesPrior/Evidence\n",
    "\t\t\t\t\t\t\t\tcumulativeDist=np.cumsum(sorted(Posterior))\n",
    "\t\t\t\t\t\t\t\trandomNumber=np.random.random()\n",
    "\t\t\t\t\t\t\t\tfor _ in range(len(T)):\n",
    "\t\t\t\t\t\t\t\t\tif cumulativeDist[_]>randomNumber:\n",
    "\t\t\t\t\t\t\t\t\t\tselectedLetterIndex=_\n",
    "\t\t\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t\t\tfoundLetters[i]=(list(letter2idx.keys())[selectedLetterIndex])\n",
    "\t\t\t\t\t\t\t\t#knownLetterIndeces.append(i)\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tPosterior=TMatrix[letter2idx[text[(i-1)]],:]\n",
    "\t\t\t\t\t\t\t\t#print(Posterior)\n",
    "\t\t\t\t\t\t\t\tcumulativeDist=np.cumsum(sorted(Posterior))\n",
    "\t\t\t\t\t\t\t\trandomNumber=np.random.random()\n",
    "\t\t\t\t\t\t\t\tfor _ in range(len(T)):\n",
    "\t\t\t\t\t\t\t\t\tif cumulativeDist[_]>randomNumber:\n",
    "\t\t\t\t\t\t\t\t\t\tselectedLetterIndex=_\n",
    "\t\t\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t\t\tfoundLetters[i]=(list(letter2idx.keys())[selectedLetterIndex])\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tnextKnownLetterIndex=[nextIndex for nextIndex in knownLetterIndeces if nextIndex>i]\n",
    "\t\t\t\t\t\t\tif len(nextKnownLetterIndex)>0:\n",
    "\t\t\t\t\t\t\t\tnextKnownLetterIndex=nextKnownLetterIndex[0]\n",
    "\t\t\t\t\t\t\t\tTransitionMat=np.identity(len(T))\n",
    "\t\t\t\t\t\t\t\tfor _ in range(nextKnownLetterIndex-i):\n",
    "\t\t\t\t\t\t\t\t\tTransitionMat=np.matmul(TransitionMat,TMatrix)\n",
    "\t\t\t\t\t\t\t\tLikelihoodTimesPrior=np.multiply(TMatrix[letter2idx[foundLetters[(i-1)]],:],np.transpose(TransitionMat[:,letter2idx[text[nextKnownLetterIndex]]]))\n",
    "\t\t\t\t\t\t\t\tEvidence=np.matmul(TransitionMat,TMatrix)[letter2idx[foundLetters[(i-1)]],letter2idx[text[nextKnownLetterIndex]]]\n",
    "\t\t\t\t\t\t\t\tPosterior=LikelihoodTimesPrior/Evidence\n",
    "\t\t\t\t\t\t\t\t#cumulativeDist=np.cumsum(sorted(TMatrix[letter2idx[foundLetters[i-1]],:]))\n",
    "\t\t\t\t\t\t\t\tcumulativeDist=np.cumsum(sorted(Posterior))\n",
    "\t\t\t\t\t\t\t\trandomNumber=np.random.random()\n",
    "\t\t\t\t\t\t\t\tfor _ in range(len(T)):\n",
    "\t\t\t\t\t\t\t\t\tif cumulativeDist[_]>randomNumber:\n",
    "\t\t\t\t\t\t\t\t\t\tselectedLetterIndex=_\n",
    "\t\t\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t\t\tfoundLetters[i]=(list(letter2idx.keys())[selectedLetterIndex])\n",
    "\t\t\t\t\t\t\t\t#knownLetterIndeces.append(i)\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tPosterior=TMatrix[letter2idx[foundLetters[(i-1)]],:]\n",
    "\t\t\t\t\t\t\t\tcumulativeDist=np.cumsum(sorted(Posterior))\n",
    "\t\t\t\t\t\t\t\trandomNumber=np.random.random()\n",
    "\t\t\t\t\t\t\t\tfor _ in range(len(T)):\n",
    "\t\t\t\t\t\t\t\t\tif cumulativeDist[_]>randomNumber:\n",
    "\t\t\t\t\t\t\t\t\t\tselectedLetterIndex=_\n",
    "\t\t\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t\t\tfoundLetters[i]=(list(letter2idx.keys())[selectedLetterIndex])\t\t\t\t\t\t\t\n",
    "\t\t\tcurrentText=copy.copy(text)\n",
    "\t\t\tcurrentText=list(currentText)\n",
    "\t\t\tfor _ in unknownLetterIndeces:\n",
    "\t\t\t\tcurrentText[_]=foundLetters[_]\n",
    "\t\t\tprint(''.join(currentText))\n",
    "\t\tprint('')\n",
    "\n",
    "def TextPredictor(test_strings):\n",
    "\tfor text in test_strings:\n",
    "\t\ttext='.'+text\n",
    "\t\tknownLetterIndeces=[i for i,letter in enumerate(text) if letter!='_']\n",
    "\t\tunknownLetterIndeces=[i for i,letter in enumerate(text) if letter=='_']\n",
    "\t\tfoundLetters={}\n",
    "\t\titerable=iter(enumerate(text))\n",
    "\t\tmaxProb=1\n",
    "\t\twarning=False\n",
    "\t\tfor __ in iterable:\n",
    "\t\t\ti=__[0]\n",
    "\t\t\tletter=__[1]\n",
    "\t\t\tif i in unknownLetterIndeces:\n",
    "\t\t\t\tpreviousKnownLetterIndex=[index for index in knownLetterIndeces if index<i][-1]\n",
    "\t\t\t\tnextKnownLetterIndex=[index for index in knownLetterIndeces if index>i]\n",
    "\t\t\t\tif nextKnownLetterIndex:\n",
    "\t\t\t\t\tnextKnownLetterIndex=nextKnownLetterIndex[0]\n",
    "\t\t\t\t\tunknownSequenceLength=nextKnownLetterIndex-previousKnownLetterIndex-1\n",
    "\t\t\t\t\tif unknownSequenceLength>3 and not warning:\n",
    "\t\t\t\t\t\tprint('this might take a while..')\n",
    "\t\t\t\t\t\twarning=True\n",
    "\t\t\t\t\tjointProbabilityDistribution=np.zeros(tuple(unknownSequenceLength*[len(T)]))+1\n",
    "\t\t\t\t\tpossibleCombinations=list(itertools.product(range(len(T)),repeat=unknownSequenceLength))\n",
    "\t\t\t\t\tTransitionMat=TMatrix\n",
    "\t\t\t\t\tfor _ in range(unknownSequenceLength):\n",
    "\t\t\t\t\t\tTransitionMat=np.matmul(TransitionMat,TMatrix)\n",
    "\t\t\t\t\tdenominator=TransitionMat[letter2idx[text[previousKnownLetterIndex]],letter2idx[text[nextKnownLetterIndex]]]\n",
    "\t\t\t\t\tfor comb in possibleCombinations:\n",
    "\t\t\t\t\t\tjointProbabilityDistribution[comb]*=TMatrix[letter2idx[text[previousKnownLetterIndex]],comb[0]]\n",
    "\t\t\t\t\t\tfor _ in range(unknownSequenceLength-1):\n",
    "\t\t\t\t\t\t\tjointProbabilityDistribution[comb]*=TMatrix[comb[_],comb[_+1]]\n",
    "\t\t\t\t\t\tjointProbabilityDistribution[comb]*=TMatrix[comb[-1],letter2idx[text[nextKnownLetterIndex]]]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tunknownSequenceLength=i-previousKnownLetterIndex\n",
    "\t\t\t\t\tif unknownSequenceLength>3 and not warning:\n",
    "\t\t\t\t\t\tprint('this might take a while..')\n",
    "\t\t\t\t\t\twarning=True\n",
    "\t\t\t\t\tjointProbabilityDistribution=np.zeros(tuple(unknownSequenceLength*[len(T)]))+1\n",
    "\t\t\t\t\tpossibleCombinations=list(itertools.product(range(len(T)),repeat=unknownSequenceLength))\n",
    "\t\t\t\t\tdenominator=1\n",
    "\t\t\t\t\tfor comb in possibleCombinations:\n",
    "\t\t\t\t\t\tjointProbabilityDistribution[comb]*=TMatrix[letter2idx[text[previousKnownLetterIndex]],comb[0]]\n",
    "\t\t\t\t\t\tfor _ in range(unknownSequenceLength-1):\n",
    "\t\t\t\t\t\t\tjointProbabilityDistribution[comb]*=TMatrix[comb[_],comb[_+1]]\n",
    "\t\t\t\tmaximumIndex=np.unravel_index(np.argmax(jointProbabilityDistribution, axis=None), jointProbabilityDistribution.shape)\n",
    "\t\t\t\tfor _ in range(unknownSequenceLength):\t\n",
    "\t\t\t\t\tfoundLetters[i+_]=list(letter2idx.keys())[maximumIndex[_]]\n",
    "\t\t\t\tnumerator=jointProbabilityDistribution[maximumIndex]\n",
    "\t\t\t\tfor _ in range(unknownSequenceLength-1):\n",
    "\t\t\t\t\ttemp=next(iterable,None)\n",
    "\t\t\t\tmaxProb*=(numerator/denominator)\n",
    "\t\tcurrentText=copy.copy(text)\n",
    "\t\tcurrentText=list(currentText)\n",
    "\t\tfor _ in unknownLetterIndeces:\n",
    "\t\t\tcurrentText[_]=foundLetters[_]\n",
    "\t\tdel currentText[0]\n",
    "\t\tprint('Most probable word:',''.join(currentText),'log(P(unknown|known))=',round(np.log(maxProb),4))\n",
    "\tprint('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextSampler draws 10 samples for each test string with corresponding distribution.\n",
    "\n",
    "TextPredictor fills the gaps by constructing joint probability table for the consecutive unknown letters and selecting the index for maximum probability value.\n",
    "Example:\n",
    "in text 'a_c' $P(x_2|x_1,x_3)=\\frac{P(x_2|x_1)P(x_3|x_2)}{P(x_3|x_1)}$, meaning the $x_2$ is dependent on $x_1$ and $x_3$.\n",
    "\n",
    "in text 'ac_de' $P(x_3|x_1,x_2,x_4,x_5)=\\frac{P(x_3|x_2)P(x_4|x_3)}{P(x_4|x_2)}$, meaning the $x_3$ is still dependent on $x_2$ and $x_4$.\n",
    "\n",
    "in text 'a_c_e' $P(x_2|x_1,x_3,x_5)=\\frac{P(x_2|x_1)P(x_3|x_2)}{P(x_3|x_1)}$ and $P(x_4|x_1,x_3,x_5)=\\frac {P(x_4|x_3)P(x_5|x_4)}{P(x_4|x_3)}$ \n",
    "moreover $P(x_2,x_4|x_1,x_3,x_5)=P(x_2|x_1,x_3,x_5)P(x_4|x_1,x_3,x_5)$ giving conditional independence of $x_2$ and $x_4$ meaning the estimations for $x_2$ and $x_4$ can be separated.\n",
    "\n",
    "in text 'a__d' $P(x_2|x_1,x_4)=\\frac{P(x_2|x_1)P(x_4|x_2)}{P(x_4|x_1)}$ and $P(x_3|x_1,x_4)=\\frac{P(x_3|x_1)P(x_4|x_3)}{P(x_4|x_1)}$\n",
    "moreover $P(x_2,x_3|x_1,x_4)=\\frac {P(x_2|x_1)P(x_3|x_2)P(x_4|x_3)}{P(x_4|x_1)}\\neq P(x_2|x_1,x_4)P(x_3|x_1,x_4)$ meaning the estimation for $x_2$ and $x_3$ cannot be separated and a 27x27 probability matrix must be formed to find the most probable letter combination.\n",
    "\n",
    "The consecutive unknown letters are predicted together by forming probability tables size of $27^{consecutive sequence length}$ and finding the $x_{-\\alpha}^* = \\arg\\max_{x_{-\\alpha}} p(x_{-\\alpha}|x_{\\alpha})$ for that sequence and continuing with the same operation for the next unkown letter sequence until all unkown letters are predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the.br.an.fex.\n",
      "thi.brcan.fux.\n",
      "thoubrvin.fex.\n",
      "the.br.an.fex.\n",
      "th.abr.wn.fex.\n",
      "the.brfon.fex.\n",
      "theabr.on.fix.\n",
      "the.brsen.fex.\n",
      "the.br.an.fex.\n",
      "tht.breen.fex.\n",
      "\n",
      "aulsthind.toube.answeres\n",
      "tunsthant.to.be.insworth\n",
      "putste.nd.to.be.inswored\n",
      "oulsthena.to.be.onswar.b\n",
      "butsthint.tombe.answerui\n",
      "ousst.and.to.be.answeroo\n",
      "nusst.ond.toube.tnswarom\n",
      "ouistyond.tombe.unswarsc\n",
      "ounsteand.to.be.answared\n",
      "munst.ans.tombe.onswar..\n",
      "\n",
      "ineate.pathind..ear.ing\n",
      "in.ato.nathene.peerting\n",
      "iodate.tashand.herrting\n",
      "in.atl..achene.vearring\n",
      "iswato.hathand.we.r.ing\n",
      "itsats.sathand.tear.ing\n",
      "i.cath.bathind.ce.rting\n",
      "in.ata.banhong.we.rming\n",
      "ingate.path.ng.sepr.ing\n",
      "injate.hachang.weeraing\n",
      "\n",
      "qusot..wiz.titott.be..i..fe.\n",
      "qulat.carz.se.ret.dd.upe.is.\n",
      "qur.t.thiz.de.ent.ad.gat.ar.\n",
      "qunct.w.iz.arerat.pe.tee.we.\n",
      "qun.t.bsaz.y.wact.ve..fo.sh.\n",
      "qurit.rerz.abe.nt.bs.orl.is.\n",
      "qug.t.outz.he.s.t.bl.ave.we.\n",
      "qutot.t.az.he.f.t.ck..or.ne.\n",
      "quo.t.iorz.jessct.ss.ler.ld.\n",
      "quttt.intz.then.t.an.ind.st.\n",
      "\n",
      "Most probable word: the.br.an.fex. log(P(unknown|known))= -3.0743\n",
      "Most probable word: oursthend.to.be.answeree log(P(unknown|known))= -12.2655\n",
      "Most probable word: in.ath.wathend.he.r.ing log(P(unknown|known))= -11.6361\n",
      "this might take a while..\n",
      "Most probable word: qus.t.herz.thed.t.he.the.he. log(P(unknown|known))= -21.1403\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TextSampler(test_strings,10)\n",
    "TextPredictor(test_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model could be improved by using a higher order Markov Chain, meaning in a $N^{th}$ order Markov Chain the probability of $ x_{t}$ is dependent on $ x_{t-1:t-N}$. The improved model would predict syllables instead of letters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
