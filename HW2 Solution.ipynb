{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Kaan Aytekin\n",
    "\n",
    "I hereby declare that I observed the honour code of the university when preparing the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr?gr?mm?ng?H?m?w?rk\n",
    "\n",
    "In this exercise we model a string of text using a Markov(1) model. For simplicity we only consider letters 'a-z'. Capital letters 'A-Z' are mapped to the corresponding ones. All remaining letters, symbols, numbers, including spaces, are denoted by '.'.\n",
    "\n",
    "\n",
    "We have a probability table $T$ where $T_{i,j} = p(x_t = j | x_{t-1} = i)$  transition model of letters in English text for $t=1,2 \\dots N$. Assume that the initial letter in a string is always a space denoted as $x_0 = \\text{'.'}$. Such a model where the probability table is always the same is sometimes called a stationary model.\n",
    "\n",
    "1. For a given $N$, write a program to sample random strings with letters $x_1, x_2, \\dots, x_N$ from $p(x_{1:N}|x_0)$\n",
    "1. Now suppose you are given strings with missing letters, where each missing letter is denoted by a question mark (or underscore, as below). Implement a method, that samples missing letters conditioned on observed ones, i.e., samples from $p(x_{-\\alpha}|x_{\\alpha})$ where $\\alpha$ denotes indices of observed letters. For example, if the input is 't??.', we have $N=4$ and\n",
    "$x_1 = \\text{'t'}$ and $x_4 = \\text{'.'}$, $\\alpha=\\{1,4\\}$ and $-\\alpha=\\{2,3\\}$. Your program may possibly generate the strings 'the.', 'twi.', 'tee.', etc. Hint: make sure to make use all data given and sample from the correct distribution. Implement the method and print the results for the test strings below. \n",
    "1. Describe a method for filling in the gaps by estimating the most likely letter for each position. Hint: you need to compute\n",
    "$$\n",
    "x_{-\\alpha}^* = \\arg\\max_{x_{-\\alpha}} p(x_{-\\alpha}|x_{\\alpha})\n",
    "$$\n",
    "Implement the method and print the results for the following test strings along with the log-probability  $\\log p(x_{-\\alpha}^*,x_{\\alpha})$.\n",
    "1. Discuss how you can improve the model to get better estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: The code below loads a table of transition probabilities for English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949749\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t | x_{t-1} = \\text{'a'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.0002835\n",
      "b 0.0228302\n",
      "c 0.0369041\n",
      "d 0.0426290\n",
      "e 0.0012216\n",
      "f 0.0075739\n",
      "g 0.0171385\n",
      "h 0.0014659\n",
      "i 0.0372661\n",
      "j 0.0002353\n",
      "k 0.0110124\n",
      "l 0.0778259\n",
      "m 0.0260757\n",
      "n 0.2145354\n",
      "o 0.0005459\n",
      "p 0.0195213\n",
      "q 0.0001749\n",
      "r 0.1104770\n",
      "s 0.0934290\n",
      "t 0.1317960\n",
      "u 0.0098029\n",
      "v 0.0306574\n",
      "w 0.0088799\n",
      "x 0.0009562\n",
      "y 0.0233701\n",
      "z 0.0018701\n",
      ". 0.0715219\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T = []\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        T.append(row)\n",
    "\n",
    "print('Example')\n",
    "## p(x_t = 'u' | x_{t-1} = 'q')\n",
    "display(Latex(r\"$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$\"))\n",
    "print(T[letter2idx['q']][letter2idx['u']])\n",
    "display(Latex(r\"$p(x_t | x_{t-1} = \\text{'a'})$\"))\n",
    "for c,p in zip(alphabet,T[letter2idx['a']]):\n",
    "    print(c,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import itertools\n",
    "TMatrix=np.matrix(T,dtype=float)\n",
    "FirstLetterProbs=T[len(T)-1]\n",
    "def RandomTextGenerator(N):\n",
    "\tword=(N+1)*['.']\n",
    "\tletter0='.'\n",
    "\tpreviousLetter=letter0\n",
    "\tfor i in range(N):\n",
    "\t\trandomNumber=np.random.random()\n",
    "\t\tTransitionProbs=T[letter2idx[previousLetter]]\n",
    "\t\tcumulativeDist=np.cumsum(sorted(TransitionProbs))\n",
    "\t\tfor j in range(27):\n",
    "\t\t\tif cumulativeDist[j]>randomNumber:\n",
    "\t\t\t\tselectedLetterIndex=j\n",
    "\t\t\t\tbreak\n",
    "\t\tnextLetterIndex=np.argsort(TransitionProbs)[j]\n",
    "\t\tnextLetter=list(letter2idx.keys())[nextLetterIndex]\n",
    "\t\tword[i+1]=nextLetter\n",
    "\t\tpreviousLetter=nextLetter\n",
    "\treturn(''.join(word))\n",
    "\n",
    "def TextSampler(test_strings,PredictionMultiplier):\n",
    "\tfor text in test_strings:\n",
    "\t\tfor times in range(PredictionMultiplier):\t\n",
    "\t\t\tknownLetterIndeces=[i for i,letter in enumerate(text) if letter!='_']\n",
    "\t\t\t#knownLetterIndeces={i:letter for i,letter in enumerate(text) if letter!='_'}\n",
    "\t\t\tunknownLetterIndeces=[i for i,letter in enumerate(text) if letter=='_']\n",
    "\t\t\t#unknownLetterIndeces={i:letter for i,letter in enumerate(text) if letter=='_'}\n",
    "\t\t\tfoundLetters={}\n",
    "\t\t\tfor i,letter in enumerate(text):\n",
    "\t\t\t\tif i in unknownLetterIndeces:\n",
    "\t\t\t\t\tif i==0:\n",
    "\t\t\t\t\t\tfirstKnownLetterIndex=knownLetterIndeces[0]\n",
    "\t\t\t\t\t\tTransitionMat=np.identity(len(T))\n",
    "\t\t\t\t\t\tfor _ in range(firstKnownLetterIndex):\n",
    "\t\t\t\t\t\t\tTransitionMat=np.matmul(TransitionMat,TMatrix)\n",
    "\t\t\t\t\t\tLikelihoodTimesPrior=np.multiply(TMatrix[len(T)-1,:],np.transpose(TransitionMat[:,letter2idx[text[firstKnownLetterIndex]]]))\n",
    "\t\t\t\t\t\tEvidence=np.matmul(TransitionMat,TMatrix)[len(T)-1,letter2idx[text[firstKnownLetterIndex]]]\n",
    "\t\t\t\t\t\tPosterior=LikelihoodTimesPrior/Evidence\n",
    "\t\t\t\t\t\tcumulativeDist=np.cumsum(sorted(Posterior))\n",
    "\t\t\t\t\t\trandomNumber=np.random.random()\n",
    "\t\t\t\t\t\tfor _ in range(len(T)):\n",
    "\t\t\t\t\t\t\tif cumulativeDist[_]>randomNumber:\n",
    "\t\t\t\t\t\t\t\tselectedLetterIndex=_\n",
    "\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\tfoundLetters[i]=(list(letter2idx.keys())[selectedLetterIndex])\n",
    "\t\t\t\t\t\t#knownLetterIndeces.append(i)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tif (i-1) in knownLetterIndeces:\n",
    "\t\t\t\t\t\t\tnextKnownLetterIndex=[nextIndex for nextIndex in knownLetterIndeces if nextIndex>i]\n",
    "\t\t\t\t\t\t\tif len(nextKnownLetterIndex)>0:\n",
    "\t\t\t\t\t\t\t\tnextKnownLetterIndex=nextKnownLetterIndex[0]\n",
    "\t\t\t\t\t\t\t\tTransitionMat=np.identity(len(T))\n",
    "\t\t\t\t\t\t\t\tfor _ in range(nextKnownLetterIndex-i):\n",
    "\t\t\t\t\t\t\t\t\tTransitionMat=np.matmul(TransitionMat,TMatrix)\n",
    "\t\t\t\t\t\t\t\tLikelihoodTimesPrior=np.multiply(TMatrix[letter2idx[text[(i-1)]],:],np.transpose(TransitionMat[:,letter2idx[text[nextKnownLetterIndex]]]))\n",
    "\t\t\t\t\t\t\t\tEvidence=np.matmul(TransitionMat,TMatrix)[letter2idx[text[(i-1)]],letter2idx[text[nextKnownLetterIndex]]]\n",
    "\t\t\t\t\t\t\t\tPosterior=LikelihoodTimesPrior/Evidence\n",
    "\t\t\t\t\t\t\t\tcumulativeDist=np.cumsum(sorted(Posterior))\n",
    "\t\t\t\t\t\t\t\trandomNumber=np.random.random()\n",
    "\t\t\t\t\t\t\t\tfor _ in range(len(T)):\n",
    "\t\t\t\t\t\t\t\t\tif cumulativeDist[_]>randomNumber:\n",
    "\t\t\t\t\t\t\t\t\t\tselectedLetterIndex=_\n",
    "\t\t\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t\t\tfoundLetters[i]=(list(letter2idx.keys())[selectedLetterIndex])\n",
    "\t\t\t\t\t\t\t\t#knownLetterIndeces.append(i)\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tPosterior=TMatrix[letter2idx[text[(i-1)]],:]\n",
    "\t\t\t\t\t\t\t\t#print(Posterior)\n",
    "\t\t\t\t\t\t\t\tcumulativeDist=np.cumsum(sorted(Posterior))\n",
    "\t\t\t\t\t\t\t\trandomNumber=np.random.random()\n",
    "\t\t\t\t\t\t\t\tfor _ in range(len(T)):\n",
    "\t\t\t\t\t\t\t\t\tif cumulativeDist[_]>randomNumber:\n",
    "\t\t\t\t\t\t\t\t\t\tselectedLetterIndex=_\n",
    "\t\t\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t\t\tfoundLetters[i]=(list(letter2idx.keys())[selectedLetterIndex])\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tnextKnownLetterIndex=[nextIndex for nextIndex in knownLetterIndeces if nextIndex>i]\n",
    "\t\t\t\t\t\t\tif len(nextKnownLetterIndex)>0:\n",
    "\t\t\t\t\t\t\t\tnextKnownLetterIndex=nextKnownLetterIndex[0]\n",
    "\t\t\t\t\t\t\t\tTransitionMat=np.identity(len(T))\n",
    "\t\t\t\t\t\t\t\tfor _ in range(nextKnownLetterIndex-i):\n",
    "\t\t\t\t\t\t\t\t\tTransitionMat=np.matmul(TransitionMat,TMatrix)\n",
    "\t\t\t\t\t\t\t\tLikelihoodTimesPrior=np.multiply(TMatrix[letter2idx[foundLetters[(i-1)]],:],np.transpose(TransitionMat[:,letter2idx[text[nextKnownLetterIndex]]]))\n",
    "\t\t\t\t\t\t\t\tEvidence=np.matmul(TransitionMat,TMatrix)[letter2idx[foundLetters[(i-1)]],letter2idx[text[nextKnownLetterIndex]]]\n",
    "\t\t\t\t\t\t\t\tPosterior=LikelihoodTimesPrior/Evidence\n",
    "\t\t\t\t\t\t\t\t#cumulativeDist=np.cumsum(sorted(TMatrix[letter2idx[foundLetters[i-1]],:]))\n",
    "\t\t\t\t\t\t\t\tcumulativeDist=np.cumsum(sorted(Posterior))\n",
    "\t\t\t\t\t\t\t\trandomNumber=np.random.random()\n",
    "\t\t\t\t\t\t\t\tfor _ in range(len(T)):\n",
    "\t\t\t\t\t\t\t\t\tif cumulativeDist[_]>randomNumber:\n",
    "\t\t\t\t\t\t\t\t\t\tselectedLetterIndex=_\n",
    "\t\t\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t\t\tfoundLetters[i]=(list(letter2idx.keys())[selectedLetterIndex])\n",
    "\t\t\t\t\t\t\t\t#knownLetterIndeces.append(i)\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tPosterior=TMatrix[letter2idx[foundLetters[(i-1)]],:]\n",
    "\t\t\t\t\t\t\t\tcumulativeDist=np.cumsum(sorted(Posterior))\n",
    "\t\t\t\t\t\t\t\trandomNumber=np.random.random()\n",
    "\t\t\t\t\t\t\t\tfor _ in range(len(T)):\n",
    "\t\t\t\t\t\t\t\t\tif cumulativeDist[_]>randomNumber:\n",
    "\t\t\t\t\t\t\t\t\t\tselectedLetterIndex=_\n",
    "\t\t\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t\t\tfoundLetters[i]=(list(letter2idx.keys())[selectedLetterIndex])\t\t\t\t\t\t\t\n",
    "\t\t\tcurrentText=copy.copy(text)\n",
    "\t\t\tcurrentText=list(currentText)\n",
    "\t\t\tfor _ in unknownLetterIndeces:\n",
    "\t\t\t\tcurrentText[_]=foundLetters[_]\n",
    "\t\t\tprint(''.join(currentText))\n",
    "\t\tprint('')\n",
    "\n",
    "def TextPredictor(test_strings):\n",
    "\tfor text in test_strings:\n",
    "\t\ttext='.'+text\n",
    "\t\tknownLetterIndeces=[i for i,letter in enumerate(text) if letter!='_']\n",
    "\t\tunknownLetterIndeces=[i for i,letter in enumerate(text) if letter=='_']\n",
    "\t\tfoundLetters={}\n",
    "\t\titerable=iter(enumerate(text))\n",
    "\t\tmaxProb=1\n",
    "\t\twarning=False\n",
    "\t\tfor __ in iterable:\n",
    "\t\t\ti=__[0]\n",
    "\t\t\tletter=__[1]\n",
    "\t\t\tif i in unknownLetterIndeces:\n",
    "\t\t\t\tpreviousKnownLetterIndex=[index for index in knownLetterIndeces if index<i][-1]\n",
    "\t\t\t\tnextKnownLetterIndex=[index for index in knownLetterIndeces if index>i]\n",
    "\t\t\t\tif nextKnownLetterIndex:\n",
    "\t\t\t\t\tnextKnownLetterIndex=nextKnownLetterIndex[0]\n",
    "\t\t\t\t\tunknownSequenceLength=nextKnownLetterIndex-previousKnownLetterIndex-1\n",
    "\t\t\t\t\tif unknownSequenceLength>3 and not warning:\n",
    "\t\t\t\t\t\tprint('this might take a while..')\n",
    "\t\t\t\t\t\twarning=True\n",
    "\t\t\t\t\tjointProbabilityDistribution=np.zeros(tuple(unknownSequenceLength*[len(T)]))+1\n",
    "\t\t\t\t\tpossibleCombinations=list(itertools.product(range(len(T)),repeat=unknownSequenceLength))\n",
    "\t\t\t\t\tTransitionMat=TMatrix\n",
    "\t\t\t\t\tfor _ in range(unknownSequenceLength):\n",
    "\t\t\t\t\t\tTransitionMat=np.matmul(TransitionMat,TMatrix)\n",
    "\t\t\t\t\tdenominator=TransitionMat[letter2idx[text[previousKnownLetterIndex]],letter2idx[text[nextKnownLetterIndex]]]\n",
    "\t\t\t\t\tfor comb in possibleCombinations:\n",
    "\t\t\t\t\t\tjointProbabilityDistribution[comb]*=TMatrix[letter2idx[text[previousKnownLetterIndex]],comb[0]]\n",
    "\t\t\t\t\t\tfor _ in range(unknownSequenceLength-1):\n",
    "\t\t\t\t\t\t\tjointProbabilityDistribution[comb]*=TMatrix[comb[_],comb[_+1]]\n",
    "\t\t\t\t\t\tjointProbabilityDistribution[comb]*=TMatrix[comb[-1],letter2idx[text[nextKnownLetterIndex]]]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tunknownSequenceLength=i-previousKnownLetterIndex\n",
    "\t\t\t\t\tif unknownSequenceLength>3 and not warning:\n",
    "\t\t\t\t\t\tprint('this might take a while..')\n",
    "\t\t\t\t\t\twarning=True\n",
    "\t\t\t\t\tjointProbabilityDistribution=np.zeros(tuple(unknownSequenceLength*[len(T)]))+1\n",
    "\t\t\t\t\tpossibleCombinations=list(itertools.product(range(len(T)),repeat=unknownSequenceLength))\n",
    "\t\t\t\t\tdenominator=1\n",
    "\t\t\t\t\tfor comb in possibleCombinations:\n",
    "\t\t\t\t\t\tjointProbabilityDistribution[comb]*=TMatrix[letter2idx[text[previousKnownLetterIndex]],comb[0]]\n",
    "\t\t\t\t\t\tfor _ in range(unknownSequenceLength-1):\n",
    "\t\t\t\t\t\t\tjointProbabilityDistribution[comb]*=TMatrix[comb[_],comb[_+1]]\n",
    "\t\t\t\tmaximumIndex=np.unravel_index(np.argmax(jointProbabilityDistribution, axis=None), jointProbabilityDistribution.shape)\n",
    "\t\t\t\tfor _ in range(unknownSequenceLength):\t\n",
    "\t\t\t\t\tfoundLetters[i+_]=list(letter2idx.keys())[maximumIndex[_]]\n",
    "\t\t\t\tnumerator=jointProbabilityDistribution[maximumIndex]\n",
    "\t\t\t\tfor _ in range(unknownSequenceLength-1):\n",
    "\t\t\t\t\ttemp=next(iterable,None)\n",
    "\t\t\t\tmaxProb*=(numerator/denominator)\n",
    "\t\tcurrentText=copy.copy(text)\n",
    "\t\tcurrentText=list(currentText)\n",
    "\t\tfor _ in unknownLetterIndeces:\n",
    "\t\t\tcurrentText[_]=foundLetters[_]\n",
    "\t\tdel currentText[0]\n",
    "\t\tprint('Most probable word:',''.join(currentText),'log(P(unknown|known))=',round(np.log(maxProb),4))\n",
    "\tprint('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 samples are drawn for each test string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "therbroun.fex.\n",
      "the.brhon.fex.\n",
      "the.brzan.fex.\n",
      "thambrd.n.fix.\n",
      "the.br.in.fex.\n",
      "tho.brven.fex.\n",
      "the.br.in.fex.\n",
      "theabroon.fex.\n",
      "the.brman.fax.\n",
      "thiobrean.fex.\n",
      "\n",
      "munsthand.toube.inswere.\n",
      "butst.ons.tombe.answeran\n",
      "qugsthink.toube.answaren\n",
      "cu.staknd.to.be.inswerth\n",
      "ouastheny.to.be.inswer.b\n",
      "bu.string.to.be.inswerer\n",
      "bupsthind.to.be.inswerth\n",
      "buasthind.toube.unsweran\n",
      "ouastheng.to.be.inswormu\n",
      "buestrenm.toube.onsweric\n",
      "\n",
      "ithath..aghind.me.rding\n",
      "is.aty.ba.hend.rearaing\n",
      "ismat..wa.hond.he.r.ing\n",
      "incath.pashend.vearaing\n",
      "id.ate.wathend.betr.ing\n",
      "inkate.wathong.herruing\n",
      "iewatt.eaphen..wepreing\n",
      "it.ath.mashand.he.r.ing\n",
      "in.ati.pathend.befr.iag\n",
      "iovate.mathang.jetrhing\n",
      "\n",
      "qur.t..wiz.twid.t.wr.hai.me.\n",
      "quprt.burz.omy.at.t..ice.ct.\n",
      "qupet.aurz.d.f.st.ve.nsu.be.\n",
      "qum.t.wirz.fe.act.se.n.t.be.\n",
      "qunat.harz.in.ntt.wa.wat.tu.\n",
      "qut.t.thiz.tfly.t.on.wly.go.\n",
      "qus.t.t.az.ofouct.pe.she.be.\n",
      "qurut.in.z.oo.s.t..g.f.h.ce.\n",
      "qun.t.torz.thicat.ce.orn.w..\n",
      "qud.t.runz...a..t.es.d.s.it.\n",
      "\n",
      "Most probable word: the.br.an.fex. log(P(unknown|known))= -3.0743\n",
      "Most probable word: oursthend.to.be.answeree log(P(unknown|known))= -12.2655\n",
      "Most probable word: in.ath.wathend.he.r.ing log(P(unknown|known))= -11.6361\n",
      "this might take a while..\n",
      "Most probable word: qus.t.herz.thed.t.he.the.he. log(P(unknown|known))= -21.1403\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TextSampler(test_strings,10)\n",
    "TextPredictor(test_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model could be improved by using a higher order Markov Chain, meaning in a $N^{th}$ order Markov Chain the probability of $ x_{t}$ is dependent on $ x_{t-1:t-N}$. The improved model would predict syllables instead of letters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
